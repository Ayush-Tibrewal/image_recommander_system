{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import pandas as pd  \n",
    "\n",
    "\n",
    "def test_git_inference_multiple_image(image_path, model_name, prefix):\n",
    "    param = {}\n",
    "    cp = []\n",
    "    id = []\n",
    "    if File.isfile(f'aux_data/models/{model_name}/parameter.yaml'):\n",
    "        param = load_from_yaml_file(f'aux_data/models/{model_name}/parameter.yaml')\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "    for image_name in os.listdir(image_path):\n",
    "      id.append(image_name)\n",
    "      multiple_image_path = image_path + image_name\n",
    "      if isinstance(multiple_image_path, str):\n",
    "          if (multiple_image_path.endswith(\".jpg\")):\n",
    "              multiple_images = [multiple_image_path]\n",
    "\n",
    "      # if it is more than 1 image, it is normally a video with multiple image\n",
    "      # frames\n",
    "      img = [load_image_by_pil(i) for i in multiple_images]\n",
    "\n",
    "      transforms = get_image_transform(param)\n",
    "      img = [transforms(i) for i in img]\n",
    "\n",
    "      # model\n",
    "      model = get_git_model(tokenizer, param)\n",
    "      pretrained = f'output/{model_name}/snapshot/model.pt'\n",
    "      checkpoint = torch_load(pretrained)['model']\n",
    "      load_state_dict(model, checkpoint)\n",
    "      model.cuda()\n",
    "      model.eval()\n",
    "      img = [i.unsqueeze(0).cuda() for i in img]\n",
    "\n",
    "      # prefix\n",
    "      max_text_len = 40\n",
    "      prefix_encoding = tokenizer(prefix,\n",
    "                                  padding='do_not_pad',\n",
    "                                  truncation=True,\n",
    "                                  add_special_tokens=False,\n",
    "                                  max_length=max_text_len)\n",
    "      payload = prefix_encoding['input_ids']\n",
    "      if len(payload) > max_text_len - 2:\n",
    "          payload = payload[-(max_text_len - 2):]\n",
    "      input_ids = [tokenizer.cls_token_id] + payload\n",
    "\n",
    "      with torch.no_grad():\n",
    "          result = model({\n",
    "              'image': img,\n",
    "              'prefix': torch.tensor(input_ids).unsqueeze(0).cuda(),\n",
    "          })\n",
    "      cap = tokenizer.decode(result['predictions'][0].tolist(), skip_special_tokens=True)\n",
    "      cp.append(cap)\n",
    "      \n",
    "    # dictionary of lists  \n",
    "    dict = {'image_id': id, 'caption': cp}  \n",
    "    df = pd.DataFrame(dict) \n",
    "    # saving the dataframe \n",
    "    df.to_csv('output.csv') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sn_pj_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3292ce013454e00b22d0841a4891479dd1bd6f787583e9e5db2e1ca9526a9df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
